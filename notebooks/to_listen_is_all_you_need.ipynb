{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building sound transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building sound transformers\n",
    "### with Whisper\n",
    "#### part 1 preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>license</th>\n",
       "      <th>rating</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>39.2297</td>\n",
       "      <td>118.1987</td>\n",
       "      <td>Muscicapa dauurica</td>\n",
       "      <td>Asian Brown Flycatcher</td>\n",
       "      <td>Matt Slaymaker</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://www.xeno-canto.org/134896</td>\n",
       "      <td>asbfly/XC134896.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asbfly</td>\n",
       "      <td>[]</td>\n",
       "      <td>['song']</td>\n",
       "      <td>51.4030</td>\n",
       "      <td>104.6401</td>\n",
       "      <td>Muscicapa dauurica</td>\n",
       "      <td>Asian Brown Flycatcher</td>\n",
       "      <td>Magnus Hellström</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>https://www.xeno-canto.org/164848</td>\n",
       "      <td>asbfly/XC164848.ogg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary_label secondary_labels      type  latitude  longitude  \\\n",
       "0        asbfly               []  ['call']   39.2297   118.1987   \n",
       "1        asbfly               []  ['song']   51.4030   104.6401   \n",
       "\n",
       "      scientific_name             common_name            author  \\\n",
       "0  Muscicapa dauurica  Asian Brown Flycatcher    Matt Slaymaker   \n",
       "1  Muscicapa dauurica  Asian Brown Flycatcher  Magnus Hellström   \n",
       "\n",
       "                                             license  rating  \\\n",
       "0  Creative Commons Attribution-NonCommercial-Sha...     5.0   \n",
       "1  Creative Commons Attribution-NonCommercial-Sha...     2.5   \n",
       "\n",
       "                                 url             filename  \n",
       "0  https://www.xeno-canto.org/134896  asbfly/XC134896.ogg  \n",
       "1  https://www.xeno-canto.org/164848  asbfly/XC164848.ogg  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the metadata\n",
    "metadata = pd.read_csv('./subsample/original_train_metadata.csv')\n",
    "\n",
    "# Display the first few rows of the metadata\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the audio files to convert them into mel spectrograms.\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "def preprocess_ogg(file_path, target_sample_rate=16000, target_length=3000):\n",
    "    \"\"\"\n",
    "    Preprocesses an .ogg file for Whisper.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the .ogg file.\n",
    "        target_sample_rate (int): Target sample rate for the waveform.\n",
    "        target_length (int): Target length for the mel spectrogram.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed mel spectrogram tensor.\n",
    "    \"\"\"\n",
    "    # Load .ogg file using soundfile\n",
    "    waveform, sample_rate = sf.read(file_path)\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    waveform = torch.tensor(waveform).float()\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if waveform.ndimension() > 1 and waveform.shape[1] > 1:\n",
    "        waveform = waveform.mean(dim=1, keepdim=True)\n",
    "    \n",
    "    # Resample to target sample rate if necessary\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = T.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Ensure waveform is 1D\n",
    "    if waveform.ndimension() > 1:\n",
    "        waveform = waveform.squeeze()\n",
    "    \n",
    "    # Convert waveform to mel spectrogram\n",
    "    mel_spectrogram = T.MelSpectrogram(sample_rate=target_sample_rate, n_mels=80)(waveform)\n",
    "    \n",
    "    # Ensure mel spectrogram is 2D\n",
    "    if mel_spectrogram.ndimension() > 2:\n",
    "        mel_spectrogram = mel_spectrogram.squeeze()\n",
    "    \n",
    "    # Pad or truncate to the target length\n",
    "    if mel_spectrogram.size(1) < target_length:\n",
    "        # Pad with zeros\n",
    "        padding = target_length - mel_spectrogram.size(1)\n",
    "        mel_spectrogram = torch.nn.functional.pad(mel_spectrogram, (0, padding))\n",
    "    else:\n",
    "        # Truncate to target length\n",
    "        mel_spectrogram = mel_spectrogram[:, :target_length]\n",
    "    \n",
    "    return mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mel spec shape is torch.Size([80, 3000])\n"
     ]
    }
   ],
   "source": [
    "# test the function\n",
    "mel_spectrogram = preprocess_ogg('./subsample/train/comgre/XC507426.ogg')\n",
    "\n",
    "print(\"mel spec shape is\", mel_spectrogram.shape) #expects torch.Size([80, 3000] as whispe expects fixed num of num_mel_channels at 80 - 80 mel frequency bing and For 30 seconds of audio, this will be 3000 frames because Whisper processes audio in frames of 10 ms (16,000 Hz audio divided into 160-sample frames).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a custom dataset and dataloader to load the audio files and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, folder_path, label_dict, num_files_per_label=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.label_dict = label_dict\n",
    "        self.num_files_per_label = num_files_per_label\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        self._load_files()\n",
    "\n",
    "    def _load_files(self):\n",
    "        for label in os.listdir(self.folder_path):\n",
    "            label_path = os.path.join(self.folder_path, label)\n",
    "            if os.path.isdir(label_path) and label in self.label_dict:\n",
    "                files = [os.path.join(label_path, f) for f in os.listdir(label_path) if f.endswith('.ogg')]\n",
    "                if self.num_files_per_label:\n",
    "                    files = files[:self.num_files_per_label]\n",
    "                self.file_paths.extend(files)\n",
    "                self.labels.extend([self.label_dict[label]] * len(files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        mel_spectrogram = preprocess_ogg(file_path)\n",
    "        return mel_spectrogram, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'comgre': 0, 'commoo3': 1, 'comsan': 2, 'eucdov': 3, 'eurcoo': 4, 'graher1': 5, 'grnsan': 6, 'lirplo': 7, 'litgre1': 8, 'rorpar': 9}\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary to map the labels to integers from the folder for noqw\n",
    "\n",
    "label_dict = {label: i for i, label in enumerate(os.listdir('./subsample/train'))}\n",
    "\n",
    "print(label_dict)\n",
    "\n",
    "#label_dict = {label: i for i, label in enumerate(metadata['primary_label'].unique())}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mel Spectrograms shape: torch.Size([2, 80, 3000])\n",
      "Labels shape: torch.Size([2])\n",
      "Mel Spectrograms shape: torch.Size([2, 80, 3000])\n",
      "Labels shape: torch.Size([2])\n",
      "Mel Spectrograms shape: torch.Size([2, 80, 3000])\n",
      "Labels shape: torch.Size([2])\n",
      "Mel Spectrograms shape: torch.Size([2, 80, 3000])\n",
      "Labels shape: torch.Size([2])\n",
      "Mel Spectrograms shape: torch.Size([2, 80, 3000])\n",
      "Labels shape: torch.Size([2])\n",
      "Mel Spectrograms shape: torch.Size([2, 80, 3000])\n",
      "Labels shape: torch.Size([2])\n",
      "Mel Spectrograms shape: torch.Size([2, 80, 3000])\n",
      "Labels shape: torch.Size([2])\n",
      "Mel Spectrograms shape: torch.Size([2, 80, 3000])\n",
      "Labels shape: torch.Size([2])\n",
      "Mel Spectrograms shape: torch.Size([2, 80, 3000])\n",
      "Labels shape: torch.Size([2])\n",
      "Mel Spectrograms shape: torch.Size([2, 80, 3000])\n",
      "Labels shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "folder_path = \"./subsample/train/\"\n",
    "label_dict = label_dict \n",
    "dataset = AudioDataset(folder_path, label_dict, num_files_per_label=2)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoader and print the batches\n",
    "for batch in data_loader:\n",
    "    mel_spectrograms, labels = batch\n",
    "    print(\"Mel Spectrograms shape:\", mel_spectrograms.shape)\n",
    "    print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create the magick, whisper for encoder and custom classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barbora.filova\\AppData\\Local\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperModel, WhisperProcessor\n",
    "\n",
    "# Load pretrained Whisper model\n",
    "model = WhisperModel.from_pretrained(\"openai/whisper-small\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullModel(\n",
       "  (encoder): WhisperEncoder(\n",
       "    (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (embed_positions): Embedding(1500, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x WhisperEncoderLayer(\n",
       "        (self_attn): WhisperSdpaAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): GELUActivation()\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (classification_block): ClassificationBlock(\n",
       "    (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "    (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_dim=128):\n",
    "        super(FullModel, self).__init__()\n",
    "        # Load the pretrained Whisper encoder\n",
    "        whisper_model = WhisperModel.from_pretrained(\"openai/whisper-small\")\n",
    "        self.encoder = whisper_model.encoder  # Extract only the encoder\n",
    "        self.classification_block = ClassificationBlock(\n",
    "            input_dim=self.encoder.config.hidden_size,  # Use encoder's hidden size\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_classes=num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get encoder outputs (assume x is already preprocessed and in the correct format)\n",
    "        encoder_outputs = self.encoder(x, output_hidden_states=True)\n",
    "        hidden_states = encoder_outputs.last_hidden_state  # Extract the last hidden state\n",
    "\n",
    "        # Pass the encoder outputs through the classification block\n",
    "        logits = self.classification_block(hidden_states)\n",
    "        return logits\n",
    "\n",
    "class ClassificationBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(ClassificationBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg_pool(x.permute(0, 2, 1)).squeeze(-1)  # Average pooling\n",
    "        x = F.relu(self.fc1(x))  # First linear layer + ReLU\n",
    "        x = self.fc2(x)  # Second linear layer\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = len(label_dict)\n",
    "\n",
    "# create the full model\n",
    "model = FullModel(num_classes=num_classes)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#create full model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cbj4df8r) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">after the fatal removal</strong> at: <a href='https://wandb.ai/liquid-candidate-personal/to%20list%20is%20all%20you%20need/runs/cbj4df8r' target=\"_blank\">https://wandb.ai/liquid-candidate-personal/to%20list%20is%20all%20you%20need/runs/cbj4df8r</a><br/> View project at: <a href='https://wandb.ai/liquid-candidate-personal/to%20list%20is%20all%20you%20need' target=\"_blank\">https://wandb.ai/liquid-candidate-personal/to%20list%20is%20all%20you%20need</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241128_223957-cbj4df8r\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:cbj4df8r). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\py_crack\\MLX_wk7\\mlx-whisper\\notebooks\\wandb\\run-20241128_224548-oxc2bue9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liquid-candidate-personal/to%20list%20is%20all%20you%20need/runs/oxc2bue9' target=\"_blank\">after the fatal removal</a></strong> to <a href='https://wandb.ai/liquid-candidate-personal/to%20list%20is%20all%20you%20need' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liquid-candidate-personal/to%20list%20is%20all%20you%20need' target=\"_blank\">https://wandb.ai/liquid-candidate-personal/to%20list%20is%20all%20you%20need</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liquid-candidate-personal/to%20list%20is%20all%20you%20need/runs/oxc2bue9' target=\"_blank\">https://wandb.ai/liquid-candidate-personal/to%20list%20is%20all%20you%20need/runs/oxc2bue9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.2312929630279541\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"to list is all you need\", name=\"after the fatal removal\")\n",
    "\n",
    "# Initialize Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FullModel(num_classes=len(label_dict)).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        mel_spectrograms, labels = batch\n",
    "        mel_spectrograms = mel_spectrograms.to(device)  # Move data to device\n",
    "        labels = labels.long().to(device)              # Ensure labels are of type long\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(mel_spectrograms)  # Combined forward pass through encoder and classification block\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Log batch loss to WandB\n",
    "        wandb.log({\"Train Batch Loss\": loss.item()})\n",
    "       \n",
    "    \n",
    "    # Calculate and log epoch loss\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\")\n",
    "    wandb.log({\"Epoch Loss\": epoch_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'model_full.pth')\n",
    "\n",
    "# model = FullModel(num_classes=num_classes)\n",
    "# model.load_state_dict(torch.load('model_full.pth'))\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barbora.filova\\AppData\\Local\\Temp\\ipykernel_36032\\2639010833.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('model1.pth')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel1.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Load weights into the encoder and classification block\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m encoder\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     20\u001b[0m classification_block\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_block\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# load to device\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'encoder'"
     ]
    }
   ],
   "source": [
    "#### Let's add inference\n",
    "# this part is from saved model\n",
    "\n",
    "# Ensure the model is moved to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "index_to_class = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_ogg_for_inference(file_path, target_sample_rate=16000, target_length=3000):\n",
    "    return preprocess_ogg(file_path, target_sample_rate, target_length)\n",
    "\n",
    "def predict(file_path):\n",
    "    # Preprocess the sample\n",
    "    mel_spectrogram = preprocess_ogg_for_inference(file_path)\n",
    "    mel_spectrogram = mel_spectrogram.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    \n",
    "    # Get encoder output\n",
    "    with torch.no_grad():\n",
    "        logits = model(mel_spectrogram)\n",
    "    \n",
    "    # Convert logits to probabilities using softmax\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the predicted class\n",
    "    predicted_class_index = torch.argmax(probabilities, dim=-1).item()\n",
    "    predicted_class_name = index_to_class[predicted_class_index]\n",
    "    \n",
    "    return predicted_class_index, predicted_class_name, probabilities.detach().cpu().numpy()\n",
    "\n",
    "# Example usage\n",
    "file_paths = [\n",
    "    \"./subsample/train/comgre/XC507426.ogg\",\n",
    "    \"./subsample/train/comsan/XC367395.ogg\",\n",
    "    # Add more file paths as needed\n",
    "]\n",
    "for file_path in file_paths:\n",
    "    predicted_class_index, predicted_class_name, probabilities = predict(file_path)\n",
    "    print(f\"File: {file_path}\")\n",
    "    print(f\"Predicted class index: {predicted_class_index}\")\n",
    "    print(f\"Predicted class name: {predicted_class_name}\")\n",
    "    print(f\"Probabilities: {probabilities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'encoder': encoder.state_dict(),\n",
    "    'classification_block': classification_block.state_dict()\n",
    "}, 'model1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./subsample/train/comgre/XC507426.ogg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./subsample/train/comsan/XC367395.ogg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Add more file paths as needed\u001b[39;00m\n\u001b[0;32m      6\u001b[0m ]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_paths:\n\u001b[1;32m----> 8\u001b[0m     predicted_class_index, predicted_class_name, probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted class index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_class_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 18\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Get encoder output\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 18\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m(mel_spectrogram)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Pass the encoder output through the classification block\u001b[39;00m\n\u001b[0;32m     21\u001b[0m logits \u001b[38;5;241m=\u001b[39m classification_block(encoder_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "file_paths = [\n",
    "    \"./subsample/train/comgre/XC507426.ogg\",\n",
    "    \"./subsample/train/comsan/XC367395.ogg\",\n",
    "    # Add more file paths as needed\n",
    "]\n",
    "for file_path in file_paths:\n",
    "    predicted_class_index, predicted_class_name, probabilities = predict(file_path)\n",
    "    print(f\"File: {file_path}\")\n",
    "    print(f\"Predicted class index: {predicted_class_index}\")\n",
    "    print(f\"Predicted class name: {predicted_class_name}\")\n",
    "    print(f\"Probabilities: {probabilities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test dataset and data loader\n",
    "test_folder_path = \"./subsample/test/\"\n",
    "test_dataset = AudioDataset(test_folder_path, label_dict, num_files_per_label=10)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "def validate(model, classification_block, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    classification_block.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            mel_spectrograms, labels = batch\n",
    "            mel_spectrograms = mel_spectrograms.to(device)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            encoder_outputs = model.encoder(mel_spectrograms)\n",
    "            logits = classification_block(encoder_outputs.last_hidden_state)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(logits, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Get the predicted class\n",
    "            _, predicted_classes = torch.max(logits, dim=1)\n",
    "            \n",
    "            # Update correct and total predictions\n",
    "            correct_predictions += (predicted_classes == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    avg_loss = running_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Run validation\n",
    "val_loss, val_accuracy = validate(model, classification_block, test_data_loader, criterion, device)\n",
    "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
